{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Génération de Texte avec LSTM - Les Œuvres Complètes de Shakespeare\n",
        "\n",
        "Ce notebook implémente un modèle de génération de texte utilisant un réseau de neurones LSTM (Long Short-Term Memory) pour apprendre et générer du texte dans le style de Shakespeare.\n",
        "\n",
        "## Objectifs\n",
        "- Télécharger et préparer le corpus de Shakespeare\n",
        "- Construire un modèle LSTM avec couche d'embedding\n",
        "- Entraîner le modèle sur le corpus\n",
        "- Générer du nouveau texte dans le style de Shakespeare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-12 02:07:19.176590: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.16.2\n",
            "Keras version: 3.11.1\n"
          ]
        }
      ],
      "source": [
        "# Importation des bibliothèques nécessaires\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import requests\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "# Configuration\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "\n",
        "# Désactiver les avertissements\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Préparation des Données\n",
        "\n",
        "### 1.1 Téléchargement du corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Téléchargement du corpus de Shakespeare...\n",
            "Taille du corpus brut: 5359444 caractères\n",
            "Nombre de lignes: 196022\n",
            "\n",
            "Aperçu des 500 premiers caractères:\n",
            "*** START OF THE PROJECT GUTENBERG EBOOK 100 ***\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "The Complete Works of William Shakespeare\n",
            "\n",
            "by William Shakespeare\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                    Contents\n",
            "\n",
            "    THE SONNETS\n",
            "    ALL’S WELL THAT ENDS WELL\n",
            "    THE TRAGEDY OF ANTONY AND CLEOPATRA\n",
            "    AS YOU LIKE IT\n",
            "    THE COMEDY OF ERRORS\n",
            "    THE TRAGEDY OF CORIOLANUS\n",
            "    CYMBELINE\n",
            "    THE TRAGEDY OF HAMLET, PRINCE OF DENMARK\n",
            "    THE FIRST PART OF KING HENRY THE FOURTH\n",
            "    THE SECOND PART OF KING HENRY THE FOURTH\n",
            "    THE LIFE OF KING HENRY THE FIFTH\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# URL du corpus de Shakespeare sur Project Gutenberg\n",
        "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
        "\n",
        "# Téléchargement du fichier\n",
        "print(\"Téléchargement du corpus de Shakespeare...\")\n",
        "response = requests.get(url)\n",
        "response.encoding = 'utf-8'\n",
        "raw_text = response.text\n",
        "\n",
        "print(f\"Taille du corpus brut: {len(raw_text)} caractères\")\n",
        "print(f\"Nombre de lignes: {len(raw_text.splitlines())}\")\n",
        "print(\"\\nAperçu des 500 premiers caractères:\")\n",
        "print(raw_text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Nettoyage des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nettoyage du texte...\n",
            "\n",
            "Taille du corpus nettoyé: 5256937 caractères\n",
            "Nombre de mots: 963425\n",
            "\n",
            "Aperçu du texte nettoyé (500 premiers caractères):\n",
            "The Complete Works of William Shakespeare by William Shakespeare Contents THE SONNETS ALLS WELL THAT ENDS WELL THE TRAGEDY OF ANTONY AND CLEOPATRA AS YOU LIKE IT THE COMEDY OF ERRORS THE TRAGEDY OF CORIOLANUS CYMBELINE THE TRAGEDY OF HAMLET, PRINCE OF DENMARK THE FIRST PART OF KING HENRY THE FOURTH THE SECOND PART OF KING HENRY THE FOURTH THE LIFE OF KING HENRY THE FIFTH THE FIRST PART OF HENRY THE SIXTH THE SECOND PART OF KING HENRY THE SIXTH THE THIRD PART OF KING HENRY THE SIXTH KING HENRY TH\n"
          ]
        }
      ],
      "source": [
        "def clean_gutenberg_text(text):\n",
        "    \"\"\"\n",
        "    Nettoie le texte téléchargé de Project Gutenberg en retirant:\n",
        "    - Les en-têtes et pieds de page\n",
        "    - Les informations de copyright\n",
        "    - Les caractères spéciaux non pertinents\n",
        "    \"\"\"\n",
        "    # Trouver le début du texte réel (après l'en-tête)\n",
        "    # L'en-tête se termine généralement par \"*** START OF\"\n",
        "    start_markers = [\n",
        "        \"*** START OF THE PROJECT GUTENBERG EBOOK\",\n",
        "        \"***START OF THE PROJECT GUTENBERG EBOOK\",\n",
        "        \"*** START OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "    ]\n",
        "    \n",
        "    start_idx = 0\n",
        "    for marker in start_markers:\n",
        "        idx = text.find(marker)\n",
        "        if idx != -1:\n",
        "            # Trouver la fin de la ligne et prendre la ligne suivante\n",
        "            start_idx = text.find('\\n', idx) + 1\n",
        "            break\n",
        "    \n",
        "    # Trouver la fin du texte réel (avant le pied de page)\n",
        "    # Le pied de page commence généralement par \"*** END OF\"\n",
        "    end_markers = [\n",
        "        \"*** END OF THE PROJECT GUTENBERG EBOOK\",\n",
        "        \"***END OF THE PROJECT GUTENBERG EBOOK\",\n",
        "        \"*** END OF THIS PROJECT GUTENBERG EBOOK\"\n",
        "    ]\n",
        "    \n",
        "    end_idx = len(text)\n",
        "    for marker in end_markers:\n",
        "        idx = text.find(marker)\n",
        "        if idx != -1:\n",
        "            end_idx = idx\n",
        "            break\n",
        "    \n",
        "    # Extraire le texte principal\n",
        "    text = text[start_idx:end_idx]\n",
        "    \n",
        "    # Nettoyer les caractères spéciaux et normaliser les espaces\n",
        "    # Garder les sauts de ligne mais normaliser les espaces multiples\n",
        "    text = re.sub(r'[^\\w\\s\\n\\.,!?;:\\'\\\"-]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remplacer les espaces multiples par un seul\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n', text)  # Remplacer les lignes vides multiples par une seule\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "# Nettoyage du texte\n",
        "print(\"Nettoyage du texte...\")\n",
        "cleaned_text = clean_gutenberg_text(raw_text)\n",
        "\n",
        "print(f\"\\nTaille du corpus nettoyé: {len(cleaned_text)} caractères\")\n",
        "print(f\"Nombre de mots: {len(cleaned_text.split())}\")\n",
        "print(\"\\nAperçu du texte nettoyé (500 premiers caractères):\")\n",
        "print(cleaned_text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Tokenisation et préparation des séquences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulaire (mots): 27270 mots uniques\n",
            "Exemples de mots: [('the', 1), ('and', 2), ('i', 3), ('to', 4), ('of', 5), ('a', 6), ('you', 7), ('my', 8), ('in', 9), ('that', 10)]\n",
            "\n",
            "Longueur totale du texte en indices: 968004\n"
          ]
        }
      ],
      "source": [
        "# Paramètres pour la préparation des données\n",
        "SEQUENCE_LENGTH = 50  # Longueur des séquences d'entrée\n",
        "STEP = 3  # Pas pour créer les séquences (overlap)\n",
        "\n",
        "# Convertir le texte en minuscules pour réduire le vocabulaire\n",
        "text_lower = cleaned_text.lower()\n",
        "\n",
        "# Tokenisation par caractères (plus simple et efficace pour commencer)\n",
        "# Alternative: tokenisation par mots (plus complexe mais meilleure qualité)\n",
        "TOKENIZE_BY_CHAR = False  # Mettre à True pour tokeniser par caractères\n",
        "\n",
        "if TOKENIZE_BY_CHAR:\n",
        "    # Tokenisation par caractères\n",
        "    chars = sorted(list(set(text_lower)))\n",
        "    char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "    \n",
        "    vocab_size = len(chars)\n",
        "    print(f\"Vocabulaire (caractères): {vocab_size} caractères uniques\")\n",
        "    print(f\"Caractères: {''.join(chars[:50])}...\")\n",
        "    \n",
        "    # Convertir le texte en séquences d'indices\n",
        "    text_indices = [char_to_idx[char] for char in text_lower]\n",
        "else:\n",
        "    # Tokenisation par mots\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts([text_lower])\n",
        "    \n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print(f\"Vocabulaire (mots): {vocab_size} mots uniques\")\n",
        "    print(f\"Exemples de mots: {list(tokenizer.word_index.items())[:10]}\")\n",
        "    \n",
        "    # Convertir le texte en séquences d'indices\n",
        "    text_indices = tokenizer.texts_to_sequences([text_lower])[0]\n",
        "\n",
        "print(f\"\\nLongueur totale du texte en indices: {len(text_indices)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Création des séquences d'entraînement...\n",
            "Nombre de séquences créées: 322652\n",
            "Forme de X: (322652, 50)\n",
            "Forme de y: (322652,)\n",
            "y sera utilisé avec sparse_categorical_crossentropy\n",
            "\n",
            "Exemple de séquence d'entrée (10 premiers éléments): [   1 3652 2688    5 1229 9372   33 1229 9372 1482]\n",
            "Élément suivant (cible): 1513\n"
          ]
        }
      ],
      "source": [
        "# Création des séquences d'entraînement\n",
        "def create_sequences(text_indices, seq_length, step):\n",
        "    \"\"\"\n",
        "    Crée des séquences d'entrée et de sortie pour l'entraînement\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    next_chars = []\n",
        "    \n",
        "    for i in range(0, len(text_indices) - seq_length, step):\n",
        "        sequences.append(text_indices[i:i + seq_length])\n",
        "        next_chars.append(text_indices[i + seq_length])\n",
        "    \n",
        "    return np.array(sequences), np.array(next_chars)\n",
        "\n",
        "# Créer les séquences\n",
        "print(\"Création des séquences d'entraînement...\")\n",
        "X, y = create_sequences(text_indices, SEQUENCE_LENGTH, STEP)\n",
        "\n",
        "print(f\"Nombre de séquences créées: {len(X)}\")\n",
        "print(f\"Forme de X: {X.shape}\")\n",
        "print(f\"Forme de y: {y.shape}\")\n",
        "\n",
        "# Encoder y en one-hot si nécessaire (pour la tokenisation par caractères)\n",
        "# Pour les mots, on utilisera sparse_categorical_crossentropy\n",
        "if TOKENIZE_BY_CHAR:\n",
        "    y = to_categorical(y, num_classes=vocab_size)\n",
        "    print(f\"Forme de y après one-hot encoding: {y.shape}\")\n",
        "else:\n",
        "    print(f\"y sera utilisé avec sparse_categorical_crossentropy\")\n",
        "\n",
        "# Aperçu d'une séquence\n",
        "print(f\"\\nExemple de séquence d'entrée (10 premiers éléments): {X[0][:10]}\")\n",
        "print(f\"Élément suivant (cible): {y[0] if TOKENIZE_BY_CHAR else y[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Conception du Modèle LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Architecture du modèle:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Paramètres du modèle\n",
        "EMBEDDING_DIM = 256  # Dimension de l'espace d'embedding\n",
        "LSTM_UNITS = 512  # Nombre d'unités dans la couche LSTM\n",
        "DROPOUT_RATE = 0.2  # Taux de dropout pour la régularisation\n",
        "\n",
        "def build_model(vocab_size, seq_length, embedding_dim, lstm_units, dropout_rate):\n",
        "    \"\"\"\n",
        "    Construit un modèle LSTM pour la génération de texte\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        # Couche d'embedding pour convertir les indices en vecteurs denses\n",
        "        Embedding(input_dim=vocab_size, \n",
        "                  output_dim=embedding_dim, \n",
        "                  input_length=seq_length,\n",
        "                  name='embedding'),\n",
        "        \n",
        "        # Première couche LSTM avec retour de séquence\n",
        "        LSTM(lstm_units, \n",
        "             return_sequences=True,\n",
        "             name='lstm_1'),\n",
        "        Dropout(dropout_rate),\n",
        "        \n",
        "        # Deuxième couche LSTM\n",
        "        LSTM(lstm_units, \n",
        "             return_sequences=False,\n",
        "             name='lstm_2'),\n",
        "        Dropout(dropout_rate),\n",
        "        \n",
        "        # Couche de sortie dense\n",
        "        Dense(vocab_size, activation='softmax', name='output')\n",
        "    ])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Construire le modèle\n",
        "model = build_model(vocab_size, SEQUENCE_LENGTH, EMBEDDING_DIM, LSTM_UNITS, DROPOUT_RATE)\n",
        "\n",
        "# Compiler le modèle\n",
        "if TOKENIZE_BY_CHAR:\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "else:\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "# Afficher l'architecture du modèle\n",
        "print(\"Architecture du modèle:\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Entraînement du Modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Début de l'entraînement...\n",
            "Données d'entraînement: 290387 séquences\n",
            "Données de validation: 32265 séquences\n",
            "Taille des lots: 128\n",
            "Nombre d'époques: 20\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Paramètres d'entraînement\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20  # Nombre d'époques (peut être ajusté)\n",
        "VALIDATION_SPLIT = 0.1  # 10% des données pour la validation\n",
        "\n",
        "# Callbacks pour améliorer l'entraînement\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        'best_model.h5',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=0.0001,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"Début de l'entraînement...\")\n",
        "print(f\"Données d'entraînement: {len(X) * (1 - VALIDATION_SPLIT):.0f} séquences\")\n",
        "print(f\"Données de validation: {len(X) * VALIDATION_SPLIT:.0f} séquences\")\n",
        "print(f\"Taille des lots: {BATCH_SIZE}\")\n",
        "print(f\"Nombre d'époques: {EPOCHS}\")\n",
        "print(\"\\n\" + \"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m  55/2269\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m57:21\u001b[0m 2s/step - accuracy: 0.0255 - loss: 9.0010"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      3\u001b[0m     X, y,\n\u001b[1;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m      6\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39mVALIDATION_SPLIT,\n\u001b[1;32m      7\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m      8\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    376\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(begin_step)\n\u001b[0;32m--> 377\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    378\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    218\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    219\u001b[0m     ):\n\u001b[0;32m--> 220\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    222\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Entraîner le modèle\n",
        "history = model.fit(\n",
        "    X, y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=VALIDATION_SPLIT,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiser l'historique d'entraînement\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Graphique de la perte\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Perte (entraînement)')\n",
        "plt.plot(history.history['val_loss'], label='Perte (validation)')\n",
        "plt.title('Évolution de la perte')\n",
        "plt.xlabel('Époque')\n",
        "plt.ylabel('Perte')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Graphique de la précision\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Précision (entraînement)')\n",
        "plt.plot(history.history['val_accuracy'], label='Précision (validation)')\n",
        "plt.title('Évolution de la précision')\n",
        "plt.xlabel('Époque')\n",
        "plt.ylabel('Précision')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Génération de Texte\n",
        "\n",
        "### 4.1 Fonction de génération"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_text(model, seed_text, tokenizer, seq_length, num_words=100, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Génère du texte à partir d'une graine (seed)\n",
        "    \n",
        "    Parameters:\n",
        "    - model: Le modèle entraîné\n",
        "    - seed_text: Texte de départ\n",
        "    - tokenizer: Tokenizer utilisé (ou None si tokenisation par caractères)\n",
        "    - seq_length: Longueur des séquences\n",
        "    - num_words: Nombre de mots/caractères à générer\n",
        "    - temperature: Contrôle la créativité (plus élevé = plus créatif, plus bas = plus conservateur)\n",
        "    \"\"\"\n",
        "    generated_text = seed_text.lower()\n",
        "    \n",
        "    for _ in range(num_words):\n",
        "        # Préparer la séquence d'entrée\n",
        "        if TOKENIZE_BY_CHAR:\n",
        "            # Pour la tokenisation par caractères\n",
        "            sequence = [char_to_idx.get(char, 0) for char in generated_text[-seq_length:]]\n",
        "        else:\n",
        "            # Pour la tokenisation par mots\n",
        "            # Tokeniser le texte généré jusqu'à présent\n",
        "            words = generated_text.split()\n",
        "            # Prendre les derniers mots (jusqu'à seq_length)\n",
        "            recent_words = words[-seq_length:] if len(words) >= seq_length else words\n",
        "            # Convertir en séquence d'indices\n",
        "            sequence = [tokenizer.word_index.get(word, 0) for word in recent_words]\n",
        "        \n",
        "        # Pad la séquence si nécessaire\n",
        "        if len(sequence) < seq_length:\n",
        "            sequence = [0] * (seq_length - len(sequence)) + sequence\n",
        "        # Tronquer si trop longue\n",
        "        elif len(sequence) > seq_length:\n",
        "            sequence = sequence[-seq_length:]\n",
        "        \n",
        "        # Prédire le prochain élément\n",
        "        sequence = np.array(sequence).reshape(1, seq_length)\n",
        "        predictions = model.predict(sequence, verbose=0)[0]\n",
        "        \n",
        "        # Appliquer la température pour contrôler la créativité\n",
        "        predictions = np.log(predictions + 1e-10) / temperature\n",
        "        exp_predictions = np.exp(predictions)\n",
        "        predictions = exp_predictions / np.sum(exp_predictions)\n",
        "        \n",
        "        # Échantillonner selon la distribution de probabilité\n",
        "        next_index = np.random.choice(len(predictions), p=predictions)\n",
        "        \n",
        "        # Ajouter le nouvel élément au texte généré\n",
        "        if TOKENIZE_BY_CHAR:\n",
        "            next_char = idx_to_char.get(next_index, '')\n",
        "            if next_char:\n",
        "                generated_text += next_char\n",
        "        else:\n",
        "            next_word = tokenizer.index_word.get(next_index, '')\n",
        "            if next_word:\n",
        "                generated_text += ' ' + next_word\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "# Fonction pour générer du texte avec différentes graines\n",
        "def generate_samples(model, tokenizer, seq_length, seed_texts, num_words=150, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Génère plusieurs échantillons de texte avec différentes graines\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"GÉNÉRATION DE TEXTE\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for i, seed in enumerate(seed_texts, 1):\n",
        "        print(f\"\\n--- Échantillon {i} ---\")\n",
        "        print(f\"Graine: '{seed}'\")\n",
        "        print(f\"Température: {temperature}\")\n",
        "        print(\"-\" * 70)\n",
        "        generated = generate_text(model, seed, tokenizer, seq_length, num_words, temperature)\n",
        "        print(generated)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Génération avec différentes graines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Charger le meilleur modèle si disponible\n",
        "if os.path.exists('best_model.h5'):\n",
        "    print(\"Chargement du meilleur modèle sauvegardé...\")\n",
        "    model.load_weights('best_model.h5')\n",
        "\n",
        "# Préparer le tokenizer pour la génération\n",
        "if TOKENIZE_BY_CHAR:\n",
        "    gen_tokenizer = None\n",
        "else:\n",
        "    gen_tokenizer = tokenizer\n",
        "\n",
        "# Différentes graines pour tester le modèle\n",
        "seed_texts = [\n",
        "    \"To be or not to be\",\n",
        "    \"Romeo and Juliet\",\n",
        "    \"All the world's a stage\",\n",
        "    \"Double, double toil and trouble\",\n",
        "    \"What light through yonder window breaks\"\n",
        "]\n",
        "\n",
        "# Générer du texte avec température modérée (créativité équilibrée)\n",
        "generate_samples(model, gen_tokenizer, SEQUENCE_LENGTH, seed_texts, \n",
        "                 num_words=200, temperature=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Expérimentation avec différentes températures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tester différentes températures pour voir l'impact sur la créativité\n",
        "seed = \"To be or not to be\"\n",
        "temperatures = [0.5, 1.0, 1.5]\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPARAISON DES TEMPÉRATURES\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Graîne: '{seed}'\\n\")\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n--- Température: {temp} ---\")\n",
        "    generated = generate_text(model, seed, gen_tokenizer, SEQUENCE_LENGTH, \n",
        "                              num_words=150, temperature=temp)\n",
        "    print(generated)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Analyse et Conclusion\n",
        "\n",
        "### Observations\n",
        "- Le modèle apprend les patterns et le style de Shakespeare\n",
        "- La température contrôle le niveau de créativité vs cohérence\n",
        "- Plus d'époques d'entraînement améliorent généralement la qualité\n",
        "- La tokenisation par mots donne généralement de meilleurs résultats que par caractères\n",
        "\n",
        "### Améliorations possibles\n",
        "- Augmenter le nombre d'époques d'entraînement\n",
        "- Ajuster l'architecture (plus de couches LSTM, plus d'unités)\n",
        "- Utiliser des techniques avancées (attention, transformers)\n",
        "- Augmenter la taille du corpus ou utiliser un corpus plus spécialisé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sauvegarder le modèle final et le tokenizer\n",
        "print(\"Sauvegarde du modèle et des composants...\")\n",
        "\n",
        "# Sauvegarder le modèle\n",
        "model.save('shakespeare_lstm_model.h5')\n",
        "print(\"✓ Modèle sauvegardé: shakespeare_lstm_model.h5\")\n",
        "\n",
        "# Sauvegarder le tokenizer si on utilise la tokenisation par mots\n",
        "if not TOKENIZE_BY_CHAR:\n",
        "    with open('tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "    print(\"✓ Tokenizer sauvegardé: tokenizer.pkl\")\n",
        "    \n",
        "    # Sauvegarder les paramètres\n",
        "    params = {\n",
        "        'vocab_size': vocab_size,\n",
        "        'sequence_length': SEQUENCE_LENGTH,\n",
        "        'tokenize_by_char': TOKENIZE_BY_CHAR\n",
        "    }\n",
        "    with open('model_params.pkl', 'wb') as f:\n",
        "        pickle.dump(params, f)\n",
        "    print(\"✓ Paramètres sauvegardés: model_params.pkl\")\n",
        "else:\n",
        "    # Sauvegarder les mappings caractères pour la tokenisation par caractères\n",
        "    char_mappings = {\n",
        "        'char_to_idx': char_to_idx,\n",
        "        'idx_to_char': idx_to_char,\n",
        "        'vocab_size': vocab_size\n",
        "    }\n",
        "    with open('char_mappings.pkl', 'wb') as f:\n",
        "        pickle.dump(char_mappings, f)\n",
        "    print(\"✓ Mappings caractères sauvegardés: char_mappings.pkl\")\n",
        "\n",
        "print(\"\\nSauvegarde terminée!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
