{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "029cf186",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN) pour MountainCar-v0\n",
    "\n",
    "## Objectif\n",
    "Implémenter un DQN pour résoudre le problème MountainCar-v0, où l'agent doit faire monter une voiture au sommet d'une colline en utilisant son momentum.\n",
    "\n",
    "## Critère de réussite\n",
    "L'agent doit atteindre le sommet (récompense > -200) dans au moins **90%** des épisodes sur 100 épisodes consécutifs.\n",
    "\n",
    "## Architecture\n",
    "- **Réseau de neurones**: 2 couches cachées de 24 neurones chacune avec activation ReLU\n",
    "- **Experience Replay**: Mémoire tampon de 100,000 transitions\n",
    "- **Target Network**: Mise à jour tous les 10 épisodes\n",
    "- **Exploration**: Politique epsilon-greedy avec décroissance exponentielle\n",
    "- **ModelCheckpoint**: Sauvegarde automatique du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfb3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des bibliothèques nécessaires\n",
    "%pip install gym gym[classic_control] tensorflow matplotlib imageio imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa28275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import imageio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd5a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration de l'environnement\n",
    "env_name = \"MountainCar-v0\"\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "memory_size = 100000\n",
    "episodes = 500\n",
    "learning_rate = 0.001\n",
    "target_update_frequency = 10  # Mise à jour du target network tous les 10 épisodes\n",
    "success_threshold = -200  # Pour MountainCar, un épisode réussi a une récompense > -200\n",
    "success_rate_threshold = 0.90  # 90% de taux de réussite requis\n",
    "\n",
    "# Créer le dossier pour sauvegarder les modèles\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Initialisation de l'environnement\n",
    "env = gym.make(env_name)\n",
    "state_shape = env.observation_space.shape[0]\n",
    "action_shape = env.action_space.n\n",
    "\n",
    "print(f\"Environnement: {env_name}\")\n",
    "print(f\"State shape: {state_shape}\")\n",
    "print(f\"Action shape: {action_shape}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle DQN\n",
    "def create_q_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Dense(24, activation='relu', input_shape=(state_shape,)),\n",
    "        layers.Dense(24, activation='relu'),\n",
    "        layers.Dense(action_shape, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Création des réseaux principal et target\n",
    "q_model = create_q_model()\n",
    "target_model = create_q_model()\n",
    "target_model.set_weights(q_model.get_weights())\n",
    "\n",
    "# Configuration du ModelCheckpoint pour sauvegarder le meilleur modèle\n",
    "checkpoint_path = 'models/mountaincar_dqn_best.h5'\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='loss',  # On surveillera la loss pendant l'entraînement\n",
    "    save_best_only=True,\n",
    "    save_weights_only=False,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Modèles créés avec succès!\")\n",
    "print(f\"ModelCheckpoint configuré: {checkpoint_path}\")\n",
    "q_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43719920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay Buffer\n",
    "memory = deque(maxlen=memory_size)\n",
    "\n",
    "def store_transition(state, action, reward, next_state, done):\n",
    "    \"\"\"Stocke une transition dans le replay buffer\"\"\"\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def sample_batch():\n",
    "    \"\"\"Échantillonne un batch de transitions pour l'entraînement\"\"\"\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "    return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e288546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Politique epsilon-greedy\n",
    "def epsilon_greedy_policy(state, epsilon):\n",
    "    \"\"\"Sélectionne une action selon la politique epsilon-greedy\"\"\"\n",
    "    if np.random.random() < epsilon:\n",
    "        return env.action_space.sample()  # Exploration\n",
    "    else:\n",
    "        q_values = q_model.predict(state[np.newaxis], verbose=0)\n",
    "        return np.argmax(q_values[0])  # Exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd931e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entraînement\n",
    "def train_step():\n",
    "    \"\"\"Effectue une étape d'entraînement du DQN\"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    # Échantillonnage d'un batch\n",
    "    states, actions, rewards, next_states, dones = sample_batch()\n",
    "    \n",
    "    # Calcul des Q-values cibles avec le target network\n",
    "    next_q_values = target_model.predict(next_states, verbose=0)\n",
    "    max_next_q_values = np.max(next_q_values, axis=1)\n",
    "    \n",
    "    # Calcul des Q-values actuelles\n",
    "    target_q_values = q_model.predict(states, verbose=0)\n",
    "    \n",
    "    # Mise à jour des Q-values cibles selon l'équation de Bellman\n",
    "    for i in range(batch_size):\n",
    "        if dones[i]:\n",
    "            target_q_values[i][actions[i]] = rewards[i]\n",
    "        else:\n",
    "            target_q_values[i][actions[i]] = rewards[i] + gamma * max_next_q_values[i]\n",
    "    \n",
    "    # Entraînement du réseau principal\n",
    "    q_model.fit(states, target_q_values, verbose=0, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71914a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boucle d'entraînement principale\n",
    "reward_history = []\n",
    "episode_lengths = []\n",
    "average_rewards = []\n",
    "success_history = []  # Historique des succès (récompense > -200)\n",
    "success_rates = []  # Taux de réussite sur 100 épisodes\n",
    "\n",
    "# Variables pour ModelCheckpoint - sauvegarde du meilleur modèle\n",
    "best_success_rate = 0.0\n",
    "best_avg_reward = float('-inf')\n",
    "best_model_path = 'models/mountaincar_dqn_best.h5'\n",
    "\n",
    "print(\"Début de l'entraînement...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    # Gérer le nouveau format de gym (retourne un tuple avec info)\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Sélection de l'action\n",
    "        action = epsilon_greedy_policy(state, epsilon)\n",
    "        \n",
    "        # Exécution de l'action\n",
    "        result = env.step(action)\n",
    "        if len(result) == 4:  # Ancien format gym\n",
    "            next_state, reward, done, _ = result\n",
    "        else:  # Nouveau format gymnasium\n",
    "            next_state, reward, terminated, truncated, info = result\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        # Stockage de la transition\n",
    "        store_transition(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Entraînement\n",
    "        train_step()\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    \n",
    "    # Décroissance d'epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "    \n",
    "    # Mise à jour du target network\n",
    "    if episode % target_update_frequency == 0:\n",
    "        target_model.set_weights(q_model.get_weights())\n",
    "    \n",
    "    # Enregistrement des résultats\n",
    "    reward_history.append(total_reward)\n",
    "    episode_lengths.append(steps)\n",
    "    is_success = total_reward > success_threshold\n",
    "    success_history.append(is_success)\n",
    "    \n",
    "    # Calcul de la moyenne sur les 100 derniers épisodes\n",
    "    if len(reward_history) >= 100:\n",
    "        avg_reward = np.mean(reward_history[-100:])\n",
    "        average_rewards.append(avg_reward)\n",
    "        success_rate = np.mean(success_history[-100:])\n",
    "        success_rates.append(success_rate)\n",
    "        \n",
    "        # ModelCheckpoint: Sauvegarder le meilleur modèle basé sur le taux de réussite\n",
    "        if success_rate > best_success_rate or (success_rate == best_success_rate and avg_reward > best_avg_reward):\n",
    "            best_success_rate = success_rate\n",
    "            best_avg_reward = avg_reward\n",
    "            q_model.save(best_model_path)\n",
    "            if (episode + 1) % 10 == 0:  # Afficher seulement tous les 10 épisodes\n",
    "                print(f\"  → Meilleur modèle sauvegardé! (Taux: {success_rate:.2%}, Reward: {avg_reward:.2f})\")\n",
    "    else:\n",
    "        average_rewards.append(np.mean(reward_history))\n",
    "        success_rates.append(np.mean(success_history))\n",
    "    \n",
    "    # Sauvegarde périodique du modèle\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        q_model.save(f'models/mountaincar_dqn_episode_{episode + 1}.h5')\n",
    "    \n",
    "    # Affichage des résultats\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg = np.mean(reward_history[-10:])\n",
    "        recent_success_rate = np.mean(success_history[-10:]) if len(success_history) >= 10 else 0\n",
    "        print(f\"Episode: {episode + 1}/{episodes}, \"\n",
    "              f\"Reward moyen (10 derniers): {avg:.2f}, \"\n",
    "              f\"Reward moyen (100 derniers): {average_rewards[-1]:.2f}, \"\n",
    "              f\"Taux de réussite (100 derniers): {success_rates[-1]:.2%}, \"\n",
    "              f\"Epsilon: {epsilon:.3f}\")\n",
    "    \n",
    "    # Vérification du critère de réussite\n",
    "    if len(reward_history) >= 100:\n",
    "        if success_rates[-1] >= success_rate_threshold:\n",
    "            print(f\"\\n✓ Succès! Taux de réussite de {success_rates[-1]:.2%} atteint à l'épisode {episode + 1}\")\n",
    "            print(f\"  Score moyen: {average_rewards[-1]:.2f}\")\n",
    "            # Sauvegarder le modèle final\n",
    "            q_model.save('models/mountaincar_dqn_final.h5')\n",
    "            print(\"  Modèle final sauvegardé!\")\n",
    "            print(\"=\" * 50)\n",
    "            break\n",
    "\n",
    "print(\"\\nEntraînement terminé!\")\n",
    "# Sauvegarder le modèle final même si le critère n'est pas atteint\n",
    "q_model.save('models/mountaincar_dqn_final.h5')\n",
    "print(f\"Modèle final sauvegardé dans 'models/mountaincar_dqn_final.h5'\")\n",
    "print(f\"Meilleur modèle sauvegardé dans '{best_model_path}' (Taux: {best_success_rate:.2%}, Reward: {best_avg_reward:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347dea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des résultats d'entraînement\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Graphique 1: Récompenses par épisode\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(reward_history, alpha=0.6, label='Reward par épisode', color='blue')\n",
    "if len(average_rewards) > 0:\n",
    "    plt.plot(average_rewards, label='Moyenne sur 100 épisodes', color='red', linewidth=2)\n",
    "plt.axhline(y=success_threshold, color='g', linestyle='--', label=f'Seuil de réussite ({success_threshold})')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Récompense')\n",
    "plt.title('Évolution des Récompenses')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2: Taux de réussite\n",
    "plt.subplot(2, 3, 2)\n",
    "if len(success_rates) > 0:\n",
    "    plt.plot(success_rates, label='Taux de réussite (100 épisodes)', color='green', linewidth=2)\n",
    "    plt.axhline(y=success_rate_threshold, color='r', linestyle='--', \n",
    "                label=f'Seuil requis ({success_rate_threshold:.0%})')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Taux de réussite')\n",
    "plt.title('Taux de Réussite (récompense > -200)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "# Graphique 3: Longueur des épisodes\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(episode_lengths, alpha=0.6, color='purple')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Longueur de l\\'épisode')\n",
    "plt.title('Longueur des Épisodes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 4: Distribution des récompenses\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(reward_history, bins=50, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "plt.axvline(x=success_threshold, color='r', linestyle='--', label='Seuil de réussite')\n",
    "plt.xlabel('Récompense')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution des Récompenses')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 5: Évolution du taux de réussite (10 derniers)\n",
    "plt.subplot(2, 3, 5)\n",
    "if len(success_history) >= 10:\n",
    "    recent_success = [np.mean(success_history[max(0, i-9):i+1]) \n",
    "                      for i in range(len(success_history))]\n",
    "    plt.plot(recent_success, label='Taux de réussite (10 derniers)', color='orange')\n",
    "plt.xlabel('Épisode')\n",
    "plt.ylabel('Taux de réussite')\n",
    "plt.title('Taux de Réussite (fenêtre glissante 10)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1.1])\n",
    "\n",
    "# Graphique 6: Comparaison succès/échecs\n",
    "plt.subplot(2, 3, 6)\n",
    "success_count = sum(success_history)\n",
    "failure_count = len(success_history) - success_count\n",
    "plt.bar(['Succès', 'Échecs'], [success_count, failure_count], \n",
    "        color=['green', 'red'], alpha=0.7, edgecolor='black')\n",
    "plt.ylabel('Nombre d\\'épisodes')\n",
    "plt.title(f'Répartition Succès/Échecs\\n({success_count}/{len(success_history)} = {success_count/len(success_history):.1%})')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques finales\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"STATISTIQUES FINALES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Nombre d'épisodes: {len(reward_history)}\")\n",
    "print(f\"Récompense moyenne (tous épisodes): {np.mean(reward_history):.2f}\")\n",
    "if len(reward_history) >= 100:\n",
    "    print(f\"Récompense moyenne (100 derniers): {np.mean(reward_history[-100:]):.2f}\")\n",
    "print(f\"Récompense maximale: {np.max(reward_history)}\")\n",
    "print(f\"Récompense minimale: {np.min(reward_history)}\")\n",
    "print(f\"Écart-type: {np.std(reward_history):.2f}\")\n",
    "print(f\"\\nTaux de réussite global: {np.mean(success_history):.2%}\")\n",
    "if len(success_history) >= 100:\n",
    "    print(f\"Taux de réussite (100 derniers): {np.mean(success_history[-100:]):.2%}\")\n",
    "print(f\"Nombre total de succès: {sum(success_history)}/{len(success_history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le meilleur modèle sauvegardé (optionnel)\n",
    "# Décommentez les lignes suivantes pour charger un modèle précédemment sauvegardé\n",
    "\n",
    "# from tensorflow.keras.models import load_model\n",
    "# q_model = load_model('models/mountaincar_dqn_best.h5')\n",
    "# target_model = create_q_model()\n",
    "# target_model.set_weights(q_model.get_weights())\n",
    "# print(\"Meilleur modèle chargé avec succès!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'évaluation\n",
    "def evaluate_agent(env, model, num_episodes=100, render=False):\n",
    "    \"\"\"Évalue l'agent sur un nombre d'épisodes\"\"\"\n",
    "    total_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            # Action déterministe (pas d'exploration)\n",
    "            q_values = model.predict(state[np.newaxis], verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "            \n",
    "            result = env.step(action)\n",
    "            if len(result) == 4:\n",
    "                next_state, reward, done, _ = result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, info = result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            print(f\"Évaluation - Épisode {episode + 1}/{num_episodes}, \"\n",
    "                  f\"Reward moyen: {np.mean(total_rewards):.2f}\")\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "# Évaluation finale\n",
    "print(\"Évaluation de l'agent sur 100 épisodes...\")\n",
    "eval_rewards = evaluate_agent(env, q_model, num_episodes=100, render=False)\n",
    "print(f\"\\nScore moyen sur 100 épisodes: {np.mean(eval_rewards):.2f}\")\n",
    "print(f\"Score maximum: {np.max(eval_rewards)}\")\n",
    "print(f\"Score minimum: {np.min(eval_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrement d'une vidéo de l'agent entraîné\n",
    "print(\"Enregistrement d'une vidéo de l'agent...\")\n",
    "\n",
    "# Méthode 1: Utiliser gym.wrappers.RecordVideo (si disponible)\n",
    "try:\n",
    "    # Essayer avec le nouveau format gymnasium\n",
    "    try:\n",
    "        video_env = gym.make(env_name, render_mode='rgb_array')\n",
    "        video_env = gym.wrappers.RecordVideo(\n",
    "            video_env, \n",
    "            './video',\n",
    "            episode_trigger=lambda x: x == 0\n",
    "        )\n",
    "    except:\n",
    "        # Essayer avec l'ancien format gym\n",
    "        video_env = gym.make(env_name)\n",
    "        video_env = gym.wrappers.RecordVideo(\n",
    "            video_env, \n",
    "            './video',\n",
    "            episode_trigger=lambda x: x == 0\n",
    "        )\n",
    "    \n",
    "    state = video_env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    \n",
    "    done = False\n",
    "    step_count = 0\n",
    "    max_steps = 200  # MountainCar a une limite de 200 pas par défaut\n",
    "    while not done and step_count < max_steps:\n",
    "        q_values = q_model.predict(state[np.newaxis], verbose=0)\n",
    "        action = np.argmax(q_values[0])\n",
    "        \n",
    "        result = video_env.step(action)\n",
    "        if len(result) == 4:\n",
    "            next_state, reward, done, _ = result\n",
    "        else:\n",
    "            next_state, reward, terminated, truncated, info = result\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        state = next_state\n",
    "        step_count += 1\n",
    "    \n",
    "    video_env.close()\n",
    "    print(\"✓ Vidéo enregistrée dans le dossier './video'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Méthode 1 échouée: {e}\")\n",
    "    print(\"Tentative avec une méthode alternative (capture manuelle)...\")\n",
    "    \n",
    "    # Méthode 2: Capture manuelle des frames\n",
    "    try:\n",
    "        # Essayer de créer l'environnement avec render_mode\n",
    "        try:\n",
    "            env_for_video = gym.make(env_name, render_mode='rgb_array')\n",
    "        except:\n",
    "            env_for_video = gym.make(env_name)\n",
    "        \n",
    "        frames = []\n",
    "        state = env_for_video.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]\n",
    "        \n",
    "        done = False\n",
    "        step_count = 0\n",
    "        max_steps = 200  # MountainCar a une limite de 200 pas\n",
    "        \n",
    "        while not done and step_count < max_steps:\n",
    "            # Essayer de capturer le frame\n",
    "            try:\n",
    "                # Pour gymnasium/gym nouveau format\n",
    "                if hasattr(env_for_video, 'render'):\n",
    "                    frame = env_for_video.render()\n",
    "                    if frame is not None and isinstance(frame, np.ndarray):\n",
    "                        frames.append(frame)\n",
    "            except Exception as render_error:\n",
    "                # Si render() ne fonctionne pas, continuer sans frames\n",
    "                pass\n",
    "            \n",
    "            # Action de l'agent\n",
    "            q_values = q_model.predict(state[np.newaxis], verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "            \n",
    "            result = env_for_video.step(action)\n",
    "            if len(result) == 4:\n",
    "                next_state, reward, done, _ = result\n",
    "            else:\n",
    "                next_state, reward, terminated, truncated, info = result\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "        \n",
    "        env_for_video.close()\n",
    "        \n",
    "        if frames:\n",
    "            os.makedirs('video', exist_ok=True)\n",
    "            video_path = 'video/mountaincar_agent.mp4'\n",
    "            imageio.mimsave(video_path, frames, fps=30)\n",
    "            print(f\"✓ Vidéo sauvegardée: {video_path} ({len(frames)} frames)\")\n",
    "        else:\n",
    "            print(\"⚠ Impossible de capturer les frames vidéo.\")\n",
    "            print(\"  L'environnement peut nécessiter une configuration graphique spécifique.\")\n",
    "            print(\"  La vidéo sera générée automatiquement par gym.wrappers.RecordVideo si disponible.\")\n",
    "            \n",
    "    except Exception as e2:\n",
    "        print(f\"⚠ Erreur avec la méthode alternative: {e2}\")\n",
    "        print(\"  Note: L'enregistrement vidéo peut nécessiter des dépendances supplémentaires\")\n",
    "        print(\"  ou une configuration graphique. L'agent est néanmoins entraîné et fonctionnel.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
